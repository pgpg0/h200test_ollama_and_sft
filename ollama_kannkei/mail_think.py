import json
import re
from pathlib import Path
from vllm import LLM, SamplingParams
from tqdm import tqdm
import subprocess
import sys

# ===================== è¨­å®š =====================
MODEL_PATH = "/data/gpt-oss-20b"  # ãƒ­ãƒ¼ã‚«ãƒ«ã«ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹
INPUT_JSONL_PATH = Path('/home/ubuntu/client/Data_azami/code/chat_sft_data_full.jsonl')
OUTPUT_JSONL_PATH = Path('/home/ubuntu/client/okamura/thinking_results.jsonl')
FAILED_JSONL_PATH = Path('/home/ubuntu/client/okamura/failed_data.jsonl')

BATCH_SIZE = 128            # ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆãƒ¡ãƒ¢ãƒªã«å¿œã˜ã¦èª¿æ•´ï¼‰
MAX_TOKENS = 2048
TEMPERATURE = 0.4
TENSOR_PARALLEL_SIZE = 8  # GPUæšæ•°ã«åˆã‚ã›ã¦ã€‚å¿…è¦ãªã‚‰å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
UTILIZATION=0.85

# === ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã®ã¾ã¾ï¼‰ ===
THINKING_PROMPT = """
ã‚ãªãŸã¯B2Bãƒ“ã‚¸ãƒã‚¹ãƒ¡ãƒ¼ãƒ«ã®ä½œæˆã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
å—ä¿¡ã—ãŸãƒ¡ãƒ¼ãƒ«ï¼ˆquestionï¼‰ã¨ã€ãã®è¿”ä¿¡ä¾‹ï¼ˆanswerï¼‰ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚
ã“ã®è¿”ä¿¡ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ã©ã®ã‚ˆã†ãªæƒ…å ±æ•´ç†ãƒ»åˆ¤æ–­ã‚’è¡Œã£ãŸã®ã‹ã€
ãƒ¡ãƒ¼ãƒ«ä½œæˆã®æ€è€ƒéƒ¨åˆ†ã‚’æ—¥æœ¬èªã§è«–ç†çš„ã«èª¬æ˜ã—ã¦ãã ã•ã„ã€‚

# åˆ¶ç´„
- æ–‡ç« ã®ç›®çš„ã€ç›¸æ‰‹ã®è¦æœ›ã‚„è³ªå•ã®æŠŠæ¡ã€å¿…è¦ãªæƒ…å ±ã®æŠ½å‡ºã€ãƒˆãƒ¼ãƒ³ã‚„æ•¬èªã®é¸æŠãªã©ã‚’å«ã‚ã¦èª¬æ˜ã—ã¦ãã ã•ã„ã€‚
- ç®‡æ¡æ›¸ãã‚„æ®µéšçš„ãªèª¬æ˜ã§ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚
- æ€è€ƒéƒ¨åˆ†ã®å‡ºåŠ›ã®ã¿ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ï¼ˆã€Œthinking:ã€ãªã©ã®æ¥é ­è¾ã¯ä¸è¦ï¼‰ã€‚
- å®Ÿéš›ã®ãƒ¡ãƒ¼ãƒ«æ–‡é¢ã¯å«ã‚ãªã„ã“ã¨ã€‚
- æ€è€ƒéƒ¨åˆ†ã¯ä»¥ä¸‹ã®å½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™ã€‚

ã€æ€è€ƒéƒ¨åˆ†é–‹å§‹ã€‘
æ€è€ƒéƒ¨åˆ†(æ—¥æœ¬èª)
ã€æ€è€ƒéƒ¨åˆ†çµ‚äº†ã€‘

### question
{question}
### answer
{answer}

"""

# ===================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====================
def print_gpu_usage():
    try:
        result = subprocess.run(["nvidia-smi"], stdout=subprocess.PIPE, text=True)
        #print("\n=== Current GPU Usage ===")
        #print(result.stdout)
    except Exception:
        pass

def load_input_data(jsonl_file):
    data_pairs = []
    with open(jsonl_file, "r", encoding="utf-8") as f:
        for line in f:
            record = json.loads(line)
            messages = record.get("messages", [])
            user_msg = next((m for m in messages if m.get("role") == "user"), None)
            assistant_msg = next((m for m in messages if m.get("role") == "assistant"), None)
            if user_msg and assistant_msg:
                data_pairs.append({
                    "question": user_msg.get("content", "").strip(),
                    "answer": assistant_msg.get("content", "").strip(),
                    "original_entry": record
                })
    return data_pairs

def clean_llm_output(text: str) -> str:
    if not text:
        return ""
    text = text.strip()
    # å…ˆé ­ã« "thinking: " ç­‰ãŒã‚ã‚Œã°é™¤å»
    text = re.sub(r'^(thinking|æ€è€ƒ|æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹)\s*[:ï¼š]\s*', '', text, flags=re.IGNORECASE)
    return text.strip()

# ===================== ãƒãƒƒãƒç”Ÿæˆ =====================
def batch_generate(llm: LLM, tasks, batch_size=BATCH_SIZE, max_tokens=MAX_TOKENS, temperature=TEMPERATURE):
    results = []
    sampling_params = SamplingParams(temperature=temperature, max_tokens=max_tokens)

    total = len(tasks)
    pbar = tqdm(total=total, desc="ğŸ¤” Generating Thinking", unit="item")

    for i in range(0, total, batch_size):
        batch = tasks[i:i+batch_size]
        prompts = [THINKING_PROMPT.format(question=t['question'], answer=t['answer']) for t in batch]

        try:
            # ã“ã“ã§ä¸€æ‹¬ç”Ÿæˆï¼ˆvLLMã«æ¨å¥¨ã•ã‚Œã‚‹ä½¿ã„æ–¹ï¼‰
            outputs_iter = llm.generate(prompts, sampling_params)
            outputs = list(outputs_iter)  # å®‰å…¨ã®ãŸã‚å…¨å–å¾—
        except Exception as e:
            # ãƒãƒƒãƒå…¨ä½“ã®å¤±æ•—ã¨ã—ã¦ãƒãƒ¼ã‚¯
            for t in batch:
                results.append({
                    'thinking': None,
                    'original_entry': t['original_entry'],
                    'status': f'failed: {e}'
                })
            pbar.update(len(batch))
            continue

        # outputs ã¨ batch ã‚’å¯¾å¿œã•ã›ã‚‹
        for out, t in zip(outputs, batch):
            try:
                # out.outputs ã¯è¤‡æ•°ã®ãƒãƒ£ãƒ³ã‚¯è¦ç´ ã‚’å«ã‚€ã“ã¨ãŒã‚ã‚‹ãŸã‚é€£çµ
                pieces = []
                for step in getattr(out, "outputs", []):
                    # step ã«ã¯ text å±æ€§ãŒã‚ã‚‹æƒ³å®šã ãŒè¾æ›¸ã®å ´åˆã«ã‚‚å¯¾å¿œ
                    if hasattr(step, "text"):
                        pieces.append(step.text)
                    elif isinstance(step, dict) and "text" in step:
                        pieces.append(step["text"])
                generated_text = "".join(pieces).strip()
                cleaned = clean_llm_output(generated_text)
                if cleaned:
                    results.append({
                        'thinking': cleaned,
                        'original_entry': t['original_entry'],
                        'status': 'success'
                    })
                else:
                    results.append({
                        'thinking': None,
                        'original_entry': t['original_entry'],
                        'status': 'failed: empty'
                    })
            except Exception as e:
                results.append({
                    'thinking': None,
                    'original_entry': t['original_entry'],
                    'status': f'failed: {e}'
                })

            pbar.update(1)

    pbar.close()
    return results

# ===================== ä¿å­˜ =====================
def write_results(success_results, failed_results, out_path=OUTPUT_JSONL_PATH, fail_path=FAILED_JSONL_PATH):
    # success_results ã® original_entry ã® assistant ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’æ›¸ãæ›ãˆã¦ä¿å­˜
    with open(out_path, "w", encoding="utf-8") as f:
        for r in success_results:
            entry = r['original_entry']
            # assistant ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¦‹ã¤ã‘ã¦å†…å®¹ã‚’ä¸Šæ›¸ãï¼ˆå …ç‰¢ã«ï¼‰
            messages = entry.get("messages", [])
            ass_idx = next((idx for idx,m in enumerate(messages) if m.get("role")=="assistant"), 1 if len(messages)>1 else 0)
            messages[ass_idx]['thinking'] = r['thinking']
            entry['messages'] = messages
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

    with open(fail_path, "w", encoding="utf-8") as f:
        for r in failed_results:
            f.write(json.dumps({
                "status": r['status'],
                "original_entry": r['original_entry']
            }, ensure_ascii=False) + "\n")

# ===================== ãƒ¡ã‚¤ãƒ³ =====================
def main():
    print_gpu_usage()

    print(f"\nğŸ”„ Loading data from {INPUT_JSONL_PATH} ...")
    tasks = load_input_data(INPUT_JSONL_PATH)
    if not tasks:
        print("âŒ No valid tasks to process. Exiting.")
        return
    print(f"âœ… Loaded {len(tasks)} tasks.")

    print(f"\nğŸš€ Loading model from {MODEL_PATH} ... (tensor_parallel_size={TENSOR_PARALLEL_SIZE})")
    try:
        llm = LLM(model=MODEL_PATH, tensor_parallel_size=TENSOR_PARALLEL_SIZE,gpu_memory_utilization=UTILIZATION)
    except Exception as e:
        print(f"Failed to initialize vLLM LLM: {e}", file=sys.stderr)
        raise

    try:
        all_results = batch_generate(llm, tasks, batch_size=BATCH_SIZE, max_tokens=MAX_TOKENS, temperature=TEMPERATURE)
    finally:
        # æ˜ç¤ºçš„ã«ã‚¨ãƒ³ã‚¸ãƒ³çµ‚äº†ï¼ˆvLLMã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦ã¯ shutdown() / close() ãŒç„¡ã„å ´åˆã‚‚ã‚ã‚‹ï¼‰
        try:
            llm.shutdown()
        except Exception:
            pass

    successful = [r for r in all_results if r['status'] == 'success']
    failed = [r for r in all_results if r['status'] != 'success']

    print(f"\nâœï¸ Writing {len(successful)} successful results to {OUTPUT_JSONL_PATH} ...")
    write_results(successful, failed)

    print("\n--- âœ¨ Generation Complete! âœ¨ ---")
    print(f"âœ”ï¸ Successful: {len(successful)}")
    print(f"âŒ Failed:     {len(failed)}")
    print(f"ğŸ“„ Output file: {OUTPUT_JSONL_PATH}")
    print(f"ğŸ“„ Failed file: {FAILED_JSONL_PATH}")

if __name__ == "__main__":
    main()

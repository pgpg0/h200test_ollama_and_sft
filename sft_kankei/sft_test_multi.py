# âœ… Step 2: å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import torch
import json
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import LoraConfig
from trl import SFTTrainer, SFTConfig

# âœ… Step 3: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿
# Hugging Faceã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ (ä»Šåº¦ã¯æ­£ã—ãå‡¦ç†ã—ã¾ã™)
dataset = load_dataset("empower-dev/function_calling_eval_multi_turn_v0", split="train")
print("ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
print(dataset)


# ### â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…
# ### â˜…â˜…â˜…â˜…â˜… ã“ã“ãŒæœ€çµ‚çš„ãªæ­£ã—ã„å‰å‡¦ç†é–¢æ•°ã§ã™ â˜…â˜…â˜…â˜…â˜…
# ### â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…â˜…

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æ­£ã—ã„åˆ—å ('input', 'output', 'functions') ã‚’ä½¿ã£ã¦å‰å‡¦ç†ã‚’è¡Œã„ã¾ã™
def preprocess(example):
    # 1. å„åˆ—ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–ã‚Šå‡ºã™
    input_messages = example['input']
    output_message = example['output']
    functions_list = example['functions']

    # 2. åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ï¼ˆé–¢æ•°ï¼‰ã®å®šç¾©ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
    #    ã“ã‚Œã‚’ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«åŸ‹ã‚è¾¼ã‚€ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™
    tools_string = json.dumps(functions_list, indent=2)

    # 3. å…¥åŠ›ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ›¸ãæ›ãˆã‚‹
    #    (å…ƒã®ãƒ‡ãƒ¼ã‚¿ã«å½±éŸ¿ã‚’ä¸ãˆãªã„ã‚ˆã†ã« .copy() ã‚’ä½¿ç”¨)
    processed_messages = [msg.copy() for msg in input_messages]

    # 4. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆãƒªã‚¹ãƒˆã®æœ€åˆã®è¦ç´ ï¼‰ã«é–¢æ•°ã®å®šç¾©ã‚’è¿½åŠ 
    #    Qwenã®ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã§ã¯ã€ãƒ„ãƒ¼ãƒ«æƒ…å ±ã¯ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚ã‚‹ã¨åŠ¹æœçš„ã§ã™
    system_prompt = processed_messages[0]['content']
    processed_messages[0]['content'] = f"{system_prompt}\n\n## Available Tools:\n{tools_string}"

    # 5. å…¥åŠ›ã¨å‡ºåŠ›ã‚’çµåˆã—ã€1ã¤ã®å®Œå…¨ãªä¼šè©±ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ä½œæˆ
    full_conversation = processed_messages + [output_message]

    # 6. SFTTrainerãŒè¦æ±‚ã™ã‚‹ 'messages' å½¢å¼ã§è¿”ã™
    return {"messages": full_conversation}

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«æ–°ã—ã„å‰å‡¦ç†ã‚’é©ç”¨
processed_dataset = dataset.map(
    preprocess,
    remove_columns=dataset.column_names # å…ƒã® 'input', 'output', 'functions' åˆ—ã¯ä¸è¦ãªãŸã‚å‰Šé™¤
)

print("\nãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
print("å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æœ€åˆã®1ä»¶:")
#print(processed_dataset[0]['messages'])


# âœ… Step 4: ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®æº–å‚™
model_name = "Qwen/Qwen3-4B"

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

# âœ… Step 5: LoRAï¼ˆPEFTï¼‰ã®è¨­å®š
# peft_config = LoraConfig(
#     r=16,
#     lora_alpha=32,
#     lora_dropout=0.05,
#     bias="none",
#     task_type="CAUSAL_LM",
#     target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
# )

# âœ… Step 6: SFTï¼ˆå­¦ç¿’ï¼‰ã®è¨­å®š
training_args = SFTConfig(
    output_dir="./qwen3-4b-sft-final-correct",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=2,
    max_seq_length=4096, # ãƒ„ãƒ¼ãƒ«å®šç¾©ã‚’å«ã‚€ãŸã‚é•·ã‚ã«è¨­å®š
    learning_rate=5e-6,
    num_train_epochs=3,
    logging_steps=10,
    save_strategy="epoch",
    bf16=True,
    packing=True,
)

# âœ… Step 7: ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã‚’åˆæœŸåŒ–
trainer = SFTTrainer(
    model=model,
    train_dataset=processed_dataset,
    args=training_args,
)

# âœ… Step 8: å­¦ç¿’ã‚’é–‹å§‹
trainer.train()

print("ğŸ‰ å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸï¼")